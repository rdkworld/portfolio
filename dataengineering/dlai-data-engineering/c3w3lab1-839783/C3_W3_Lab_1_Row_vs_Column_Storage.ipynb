{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 3 Lab 1: Comparing the Query Performance Between Row-Oriented and Column-Oriented Databases\n",
        "\n",
        "In this lab, you will explore the performance differences between row-based and column-based databases by performing analytical queries, as well as update and delete queries, on both types of storage and comparing their execution times. Understanding the differences between these storage options will help you make informed decisions tailored to your project query needs as well as performance, scalability and cost requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Table of Contents\n",
        "\n",
        "- [ 1 - Introduction and Lab Setup](#1)\n",
        "  - [ 1.1 - Introduction](#1.1)\n",
        "  - [ 1.2 - Cloud Data Warehouse Benchmark](#1.2)\n",
        "  - [ 1.3 - Initial Imports](#1.3)\n",
        "  - [ 1.4 - Setting up the Databases and Loading the Data](#1.4)\n",
        "- [ 2 - Column-Based Database: Amazon Redshift](#2)\n",
        "  - [ 2.1 - Connecting to the Database](#2.1)\n",
        "  - [ 2.2 - Creating Queries](#2.2)\n",
        "    - [ Exercise 1](#ex01)\n",
        "    - [ Exercise 2](#ex02)\n",
        "  - [ 2.3 - Running the Benchmark Analytical Queries](#2.3)\n",
        "    - [ First TPC-H Query](#TPCHQuery1-01)\n",
        "    - [ Second TPC-H Query - Optional](#TPCHQuery1-02)\n",
        "    - [ Third TPC-H Query - Optional](#TPCHQuery1-03)\n",
        "    - [ Fourth TPC-H Query - Optional](#TPCHQuery1-04)\n",
        "    - [ Fifth TPC-H Query - Optional](#TPCHQuery1-05)\n",
        "\n",
        "  - [ 2.4 - Running the Insert and Delete Queries](#2.4)\n",
        "    - [ Exercise 3](#ex03)\n",
        "- [ 3 - Row-Based Database: PostgreSQL](#3)\n",
        "  - [ 3.1 - Connecting to the Database](#3.1)\n",
        "  - [ 3.2 - Executing the Initial Queries](#3.2)\n",
        "  - [ 3.3 - Running the Benchmark Analytical Queries](#3.3)\n",
        "    - [ First TPC-H Query](#TPCHQuery2-01)\n",
        "    - [ Second TPC-H Query - Optional](#TPCHQuery2-02)\n",
        "    - [ Third TPC-H Query - Optional](#TPCHQuery2-03)\n",
        "    - [ Fourth TPC-H Query - Optional](#TPCHQuery2-04)\n",
        "    - [ Fifth TPC-H Query - Optional](#TPCHQuery2-05)\n",
        "  - [ 3.4 - Running the Insert and Delete Queries](#3.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Introduction and Lab Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='1.1'></a>\n",
        "### 1.1 - Introduction\n",
        "\n",
        "In traditional transactional databases, the records are stored in rows which makes the databases optimized for reading and writing rows efficiently. Recently, there has been a shift towards column-oriented databases that are optimized for analytical workloads, as they are more efficient with the aggregating operations.\n",
        "\n",
        "In this lab, you are provided with an Amazon Redshift database that leverages columnar storage, and an Amazon RDS (Relational Database Service) PostgreSQL data that leverages row-oriented storage. To assess the execution time of the analytical queries, you will work with a benchmarking dataset and you will run 5 analytical queries to query the data from each store. The provided data and the corresponding SQL queries are referred to as the TPC-H benchmark. You will also write 50 rows to one table of the provided database and compare the execution time of the write query for both databases. Then you'll delete these rows from both databases and again compare the execution time of the delete query. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='1.2'></a>\n",
        "### 1.2 - Cloud Data Warehouse Benchmark\n",
        "\n",
        "You will be using [The Cloud Data Warehouse benchmark](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH), which is derived from the TPC-H Benchmark. TPC-H is a decision-support benchmark established by the Transaction Processing Performance Council (TPC) to simulate a set of basic scenarios to examine a large dataset and execute queries to answer business questions. It is designed to evaluate the performance of various database systems in how they execute complex queries. The Cloud Data Warehouse benchmark is composed of 22 queries and a database at different data scales that help simulate different analytical needs and aggregation tasks across tables. For this lab, you will be using the 10GB version of the data set, which is the smaller data scale of this benchmark. This is the entity relationship diagram of the database:\n",
        "\n",
        "![image](./images/erd.png)\n",
        "\n",
        "*Note*: The TPC Benchmark and TPC-H are trademarks of the Transaction Processing Performance Council (http://www.tpc.org). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='1.3'></a>\n",
        "### 1.3 - Initial Imports\n",
        "\n",
        "First, let's import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "import random\n",
        "import time\n",
        "import uuid\n",
        "import sqlparse\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from faker import Faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To interact with the database through a Jupyter Notebook, you will use again the magic command `%sql`. For that, run the following `load_ext` magic to load the `ipython-sql` extension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the SQL module\n",
        "%load_ext sql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each query you will test, you will need to format it using the following function: `format_query`. This function will simply add a comment with a unique random identifier for the query to be executed. This helps avoid caching results in the database and ensures getting the execution time of the query. Make sure to run the following cell to define the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_query(query: str='', path: str =None) -> str:\n",
        "    \"\"\"Takes a query or a .sql file and adds \n",
        "    a comment with a random ID to avoid DB caching\n",
        "\n",
        "    Arguments:\n",
        "        query (str): SQL query\n",
        "    \n",
        "        path (str): Path to .sql file with one query\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted query with comment\n",
        "    \n",
        "    \"\"\"\n",
        "    raw_uuid = str(uuid.uuid4()).replace('-', '')\n",
        "    query_uuid = f'view{raw_uuid}' \n",
        "    \n",
        "    if path:\n",
        "        with open(path, 'r') as file:\n",
        "            sql_commands = sqlparse.split(file.read())\n",
        "            query = sql_commands[0]\n",
        "    \n",
        "    query = query.replace(';', '')\n",
        "    sql_command = f\"/* Query ID {query_uuid} */{query};\"    \n",
        "    \n",
        "    return sql_command"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the execution time of the queries, you will be using the `%%timeit` magic command when running queries. This command provides the execution time of the query without returning any data, and can have two parameters:\n",
        "\n",
        "- `-n<N>`: Executes the given statement `<N>` times in a loop.\n",
        "- `-r<R>`: Number of repeats `<R>`, each consisting of `<N>` loops, and returns the average execution time.\n",
        "\n",
        "You can use these parameters to execute each query several times so you can get more reliable estimates for the execution times. However for time and cost constraints, you will be asked to run the queries once (`-n1` `-r1`). If you want to know more about the `timeit` package, you can take a look at the [documentation](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit).\n",
        "\n",
        "**Disclaimer:** For Redshift you will only execute the queries one time. Abstain from increasing the value of the parameters `-r` and `-n` to avoid waiting for extra time or incurring extra costs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Column-Based Database: Amazon Redshift\n",
        "\n",
        "Amazon Redshift is a fully managed data warehousing service. It is designed for large-scale data analytics and allows users to analyze their data using standard SQL queries. Redshift is based on a columnar storage architecture. This means that values from the same column are stored together on disk, allowing for efficient data compression and retrieval. It's particularly beneficial for analytics workloads where queries often access only a subset of columns.\n",
        "\n",
        "Columnar databases excel at aggregation operations (e.g., SUM, AVG, COUNT) because they only need to access the columns involved in the aggregation, rather than entire rows. This can significantly improve the performance of analytical queries, such as those commonly used in data warehousing environments.\n",
        "\n",
        "Amazon Redshift's columnar architecture makes it well-suited for analytical workloads, data warehousing, business intelligence, and ad-hoc querying, offering high performance, scalability, and cost-effectiveness when processing large volumes of data.\n",
        "\n",
        "\n",
        "Both storage solutions are already set up, with connection details available in the CloudFormation stack's output. The Amazon Redshift database is pre-filled with the benchmark data. You'll begin the lab by connecting to the Redshift cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 - Connecting to the Database\n",
        "\n",
        "Let's load some environment variables that contain the credentials to connect to the database: \n",
        "- Go to **CloudFormation** in the AWS console. You will see two stacks deployed, one associated with your Cloud9 environment (name with prefix aws-cloud9) and another named with an alphanumeric ID. \n",
        "- Click on the alphanumeric ID stack and search for the **Outputs** tab. \n",
        "- Copy the endpoints of the PostgreSQL and redshift databases that you can find under  the **value** column. Paste the endpoints in the file `./src/env` and save the changes. \n",
        "- Run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv('./src/env', override=True)\n",
        "\n",
        "REDSHIFTDBHOST = os.getenv('REDSHIFTDBHOST')\n",
        "REDSHIFTDBPORT = int(os.getenv('REDSHIFTDBPORT'))\n",
        "REDSHIFTDBNAME = os.getenv('REDSHIFTDBNAME')\n",
        "REDSHIFTDBUSER = os.getenv('REDSHIFTDBUSER')\n",
        "REDSHIFTDBPASSWORD = os.getenv('REDSHIFTDBPASSWORD')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following cell to create the connection string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "redshift_connection_url = f'redshift+psycopg2://{REDSHIFTDBUSER}:{REDSHIFTDBPASSWORD}@{REDSHIFTDBHOST}:{REDSHIFTDBPORT}/{REDSHIFTDBNAME}'\n",
        "print(redshift_connection_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Connect to the Redshift cluster by running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sql {redshift_connection_url}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the following cell to disable caching results for this session in Redshift:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sql SET enable_result_cache_for_session TO off;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you have the connection to the Redshift database established, run the following two queries to explore the tables and schema of each table in the database that you will use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT  distinct tablename\n",
        "FROM PG_TABLE_DEF\n",
        "WHERE schemaname='public'\n",
        ";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT  *\n",
        "FROM PG_TABLE_DEF\n",
        "WHERE schemaname='public'\n",
        "AND tablename='lineitem'\n",
        ";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='2.2'></a>\n",
        "### 2.2 - Creating Queries\n",
        "\n",
        "Before you test the execution times of the TPC-H benchmarking queries, you are going to issue two queries to the Redshift database. We encourage you to first explore the dataset and then develop the query to solve the following question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='ex01'></a>\n",
        "### Exercise 1\n",
        "\n",
        "Retrieve the order number, part key number, part name, customer key, order status, order date, country name and region name of the orders with the following keys: 1552449, 13620130 and 45619461.\n",
        "\n",
        "You are provided with an initial template for your query. \n",
        "\n",
        "*Note*: To see the results of the query, you will need to comment the first line that contains the magic command `%%timeit`. Then to see the execution time of the query, uncomment the magic command. \n",
        "\n",
        "Follow the instructions to complete the query:\n",
        "- In the CTE, add to the list of the `IN` operator, the keys of the orders you need to inspect (1552449, 13620130 and 45619461).\n",
        "- In the query expression, complete the `SELECT` statement with the necessary columns only:\n",
        "    - `l_orderkey`, `l_partkey`  from the `lineitemorders` table, \n",
        "    - `p_name` from the `part` table, \n",
        "    - `c_custkey` from the `customer` table, \n",
        "    - `o_orderstatus`, `o_orderdate` from the `orders` table, \n",
        "    - `n_name` from the `nation` table, \n",
        "    - `r_name` from the `region` table.\n",
        "- You need to perform 5 joins, based on the `lineitemorders` CTE:\n",
        "    - Join the `part` table on the `p_partkey` column and the `l_partkey` column from the `lineitemorders` table.\n",
        "    - Join the `orders` table on the `o_orderkey` column and the `o_orderkey` column from the `orders` table.\n",
        "    - Join the `customer` table on the `c_custkey` column and  the `o_custkey` column from the `orders` table.\n",
        "    - Join the`nation` table on the `c_nationkey` column and the `c_nationkey` column from the `customer` table\n",
        "    - Join the `region` table on the `r_regionkey` column and the `n_regionkey` column from the `nation` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "raw_sql_statement = \"\"\"    \n",
        "    WITH lineitemorders AS (\n",
        "        SELECT *\n",
        "    FROM public.lineitem\n",
        "    WHERE l_orderkey in (None, None, None)\n",
        "    )\n",
        "    \n",
        "    SELECT DISTINCT lio.None, lio.None, pt.None, ctr.None, ord.None, ord.None, ntn.None, rgn.None\n",
        "    FROM None lio\n",
        "    JOIN None pt ON pt.None = lio.None\n",
        "    JOIN None ord ON lio.None = ord.None\n",
        "    JOIN None ctr ON ctr.None = ord.None\n",
        "    JOIN None ntn ON ntn.None = ctr.None \n",
        "    JOIN None rgn ON rgn.None = ntn.None\n",
        "    ;\n",
        "\"\"\"\n",
        "\n",
        "sql_statement = format_query(query=raw_sql_statement)\n",
        "\n",
        "%sql {sql_statement}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### __Expected Output__ \n",
        "\n",
        "As a guide, your result should show `15 rows affected` which means that 15 rows have been selected in this case. Here is an example of the output.\n",
        "\n",
        "*Note*: Not all of the records are shown.\n",
        "\n",
        "\n",
        "\n",
        "| **l_orderkey** | **l_partkey** |             **p_name**             | **c_custkey** | **o_orderstatus** | **o_orderdate** |   **n_name**   | **r_name** |\n",
        "| -------------- | ------------- |----------------------------------- | ------------- | ----------------- | --------------- | -------------- | ---------- |\n",
        "| 13620130       | 733733        | snow rose salmon azure saddle      | 12461         | F                 | 1993-03-25      | ETHIOPIA       | AFRICA     |\n",
        "| 45619461       | 90452         | bisque orange black chiffon orchid | 935563        | O                 | 1995-09-07      | UNITED KINGDOM | EUROPE     |\n",
        "| 45619461       | 110840        | grey saddle firebrick tan cyan     | 935563        | O                 | 1995-09-07      | UNITED KINGDOM | EUROPE     |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='ex02'></a>\n",
        "### Exercise 2\n",
        "\n",
        "In this exercise, you will create a query that is more analytical: how many customers from the Middle East have a balance that is larger than the average balance of all customers over the same region?\n",
        "\n",
        "- Create a CTE named `avg_balance_middle_east`. In that CTE you will need to compute the average (`AVG()` function) of the customer account balance (`c_acctbal`) from the `customer` table. \n",
        "    - Join the `nation` table on the `n_nationkey` column and the `c_nationkey` column from the `customer` table.\n",
        "    - Join the `region` table on the `r_regionkey` column and the `n_regionkey` column from the `nation` table.\n",
        "    - Filter by the region name (`region.r_name`) `'MIDDLE EAST'`.\n",
        "- In the query expression, do the following:\n",
        "    - From the `customer` table, `SELECT COUNT()` the number of `DISTINCT` customers.\n",
        "    - Join again the `nation` and `region` tables on the same columns.\n",
        "    - Add two filter expressions: \n",
        "        - the first filter is to specify the region name as `'MIDDLE EAST'`; \n",
        "        - the second condition is to filter the customers that have a customer account balance (`customer.c_acctbal`) greater than the result that you computed in the `avg_balance_middle_east` CTE, so you will have to add a subquery to extract that value of `avg_balance` from the CTE in the filter.\n",
        " \n",
        "*Note*: To see the results of the query, you will need to comment the first line that contains the magic command `%%timeit`. Then uncomment the magic command and run the cell to see the execution time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "raw_sql_statement = \"\"\"    \n",
        "    WITH None AS (\n",
        "        SELECT None(None) AS None\n",
        "        FROM None\n",
        "        JOIN None ON None.None = None.None\n",
        "        JOIN None ON None.None = None.None\n",
        "        WHERE None.None = 'None EAST'\n",
        "    )\n",
        "    SELECT None(None None.None)\n",
        "    FROM None\n",
        "    JOIN None ON None.None = None.None\n",
        "    JOIN None ON None.None = None.None\n",
        "    WHERE None.None = 'None EAST'\n",
        "      AND None.None > (SELECT None FROM None);\n",
        "\"\"\"\n",
        "\n",
        "sql_statement = format_query(query=raw_sql_statement)\n",
        "\n",
        "%sql {sql_statement}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='2.3'></a>\n",
        "### 2.3 - Running the Benchmark Analytical Queries\n",
        "\n",
        "As mentioned earlier, in this lab you are using a database from the Cloud DW benchmark which is derived from TPC-H benchmark. In this part of the lab, you will execute some of the complex queries that have been developed under this benchmark. You are provided with 5 of those queries in the `./sql` folder, each query is defined in a different file. You will test the first query and the remaining four queries are optional. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='TPCHQuery1-01'></a>\n",
        "### First TPC-H Query\n",
        "\n",
        "The first analytical query that you are going to test measures the percentage of revenue (`promo_revenue`) from promotional products for orders shipped within a month. Ypu can open the file `./sql/pg_query_0_tcp_h_q14.sql` to have a look at the query.\n",
        "\n",
        "Execute the following cell to run the query: it will read the file, store the query as a string and execute the benchmark with the read query.\n",
        "\n",
        "Remember that if you want to see the actual data retrieved from this query, you should create a new cell and copy the query without the `%timeit` magic command so you will not lose any information about the execution times. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "\n",
        "sql_statement = format_query(path='./sql/pg_query_0_tcp_h_q14.sql')\n",
        "\n",
        "%sql {sql_statement}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='TPCHQuery1-02'></a>\n",
        "### Second TPC-H Query - Optional\n",
        "\n",
        "The second query is in the file `./sql/pg_query_1_tcp_h_q6.sql` and it is known as the \"Forecasting Revenue Change Query\". It computes the total revenue lost due to discounts on some line items over the year 1994.\n",
        "\n",
        "Execute the following cell to run the second query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "\n",
        "sql_statement = format_query(path='./sql/pg_query_1_tcp_h_q6.sql')\n",
        "\n",
        "%sql {sql_statement}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='TPCHQuery1-03'></a>\n",
        "### Third TPC-H Query - Optional\n",
        "\n",
        "The third query finds the top 100 orders with the highest total price where the total quantity of items is greater than 300. You can find it in the file `./sql/pg_query_2_tcp_h_q18.sql`.\n",
        "\n",
        "Run the following cell to execute the third query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "\n",
        "sql_statement = format_query(path='./sql/pg_query_2_tcp_h_q18.sql')\n",
        "\n",
        "%sql {sql_statement}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='TPCHQuery1-04'></a>\n",
        "### Fourth TPC-H Query - Optional\n",
        "\n",
        "The fourth query from the TPC-H benchmark identifies the supplier with the highest total revenue over three months. The query is in the file `./sql/pg_query_3_tcp_h_q15.sql`.\n",
        "\n",
        "Let's execute it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "\n",
        "sql_statement = format_query(path='./sql/pg_query_3_tcp_h_q15.sql')\n",
        "\n",
        "%sql {sql_statement}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='TPCHQuery1-05'></a>\n",
        "### Fifth TPC-H Query - Optional\n",
        "\n",
        "The last TPC-H benchmark query summarizes and aggregates the line item data over some time. Have a look at the content of the file `./sql/pg_query_4_tcp_h_q1.sql`.\n",
        "\n",
        "Let's execute the last TPC-H query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "\n",
        "sql_statement = format_query(path='./sql/pg_query_4_tcp_h_q1.sql')\n",
        "\n",
        "%sql {sql_statement}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Record the time that each query took as you will compare it later with the PostgreSQL (row) database. You can see that all those queries are quite complicated and try to solve complex business questions. When you compare with the row database, you will realize how a columnar database is more efficient at handling analytical queries than a row database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='2.4'></a>\n",
        "### 2.4 - Running the Insert and Delete Queries\n",
        "\n",
        "Now, you will insert and delete some data into the `lineitem` table. Remember to always execute your queries with the magic command `%timeit` to benchmark the time it took to perform the operations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='ex03'></a>\n",
        "### Exercise 3\n",
        "\n",
        "You are provided with code that creates two files: `individual_row_inserts.sql` and `individual_row_deletes.sql`. The `individual_row_inserts.sql` contains individual `INSERT` statements for each row that you will insert, while the `individual_row_deletes.sql` file contains the corresponding `DELETE` operations for each row that is on the first file. \n",
        "\n",
        "Follow the instructions to complete the code. You will use the [Faker](https://faker.readthedocs.io/en/master/) library to generate some mock data for your inserts.\n",
        "\n",
        "In this code, the `leap` variable defines how many records will be created. By default, you will generate 50 new rows.\n",
        "After completion of the code review the `insert_statement` and `delete_statement`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set a random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Generate fake data\n",
        "fake = Faker()\n",
        "\n",
        "# Define the range for l_orderkey\n",
        "leap = 50\n",
        "start_orderkey = 70000000\n",
        "end_orderkey = start_orderkey + leap\n",
        "\n",
        "# Generate SQL queries and write them to .sql files\n",
        "with open(\"./sql/individual_row_inserts.sql\", \"w\") as insert_file, open(\"./sql/individual_row_deletes.sql\", \"w\") as delete_file:\n",
        "    for orderkey in range(start_orderkey, end_orderkey):\n",
        "        \n",
        "        ### START CODE HERE ### (~ 15 lines of code)\n",
        "        \n",
        "        # Use the `random_int` method from the `fake` object to generate random integers\n",
        "        # Set a range between 1 and 1000000\n",
        "        partkey = None.None(None, None)\n",
        "        suppkey = fake.random_int(1, 10000) \n",
        "        \n",
        "        \n",
        "        # Use the `random_int` method from the `fake` object to generate random integers\n",
        "        # Set a range between 1 and 10\n",
        "        linenumber = None.None(None, None)\n",
        "        \n",
        "        quantity = round(fake.random_number(2), 2) \n",
        "        \n",
        "        extendedprice = round(fake.random_number(4), 2) \n",
        "        \n",
        "        discount = round(fake.random_number(2), 2) \n",
        "        \n",
        "        tax = round(fake.random_number(2), 2) \n",
        "        \n",
        "        returnflag = fake.random_element(elements=('N', 'R', 'A')) \n",
        "        \n",
        "        linestatus = fake.random_element(elements=('O', 'F')) \n",
        "        \n",
        "        \n",
        "        # Use the `date_between` method from the `fake` object\n",
        "        # Use start date as 1 year ago  as '-1y' and `today` as end date\n",
        "        # Chain with the `strftime` method to return a string with the format '%Y-%m-%d'\n",
        "        shipdate = None.None(start_date='None', end_date='None').None('None')\n",
        "        commitdate = (datetime.strptime(shipdate, '%Y-%m-%d') + timedelta(days=fake.random_int(1, 30))).strftime('%Y-%m-%d') \n",
        "        \n",
        "        receiptdate = (datetime.strptime(commitdate, '%Y-%m-%d') + timedelta(days=fake.random_int(1, 30))).strftime('%Y-%m-%d') \n",
        "        \n",
        "        \n",
        "        # Use the `text` method of the `fake` object to generate a text of 25 characters\n",
        "        shipinstruct = None.None(max_nb_chars=None)\n",
        "        \n",
        "        # Use the `text` method of the `fake` object to generate a text of 10 characters\n",
        "        shipmode = None.None(max_nb_chars=None)\n",
        "        \n",
        "        # Use the `text` method of the `fake` object to generate a text of 44 characters\n",
        "        comment = None.None(max_nb_chars=None)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Generate the SQL insert statement\n",
        "        insert_statement = f\"\"\"\n",
        "            INSERT INTO public.lineitem (\n",
        "                l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax,\n",
        "                l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode, l_comment\n",
        "            ) VALUES (\n",
        "                {orderkey}, {partkey}, {suppkey}, {linenumber}, {quantity}, {extendedprice}, {discount}, {tax},\n",
        "                '{returnflag}', '{linestatus}', '{shipdate}', '{commitdate}', '{receiptdate}', '{shipinstruct}', '{shipmode}', '{comment}'\n",
        "            );\n",
        "        \"\"\"\n",
        "        \n",
        "        # Write the SQL insert statement to the file\n",
        "        insert_file.write(insert_statement + \"\\n\")\n",
        "\n",
        "        # Generate the SQL delete statement\n",
        "        delete_statement = f\"\"\"\n",
        "            DELETE FROM public.lineitem\n",
        "            WHERE l_orderkey = {orderkey} AND l_linenumber = {linenumber};\n",
        "        \"\"\"\n",
        "        \n",
        "        # Write the SQL delete statement to the file\n",
        "        delete_file.write(delete_statement + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you have executed the previous cell, the two files should be in your `./sql` folder. Let's execute the insertions with the following two cells. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('./sql/individual_row_inserts.sql', 'r') as file:\n",
        "    sql_commands = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%timeit -n1 -r1 %sql $sql_commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Do you think inserting these 50 rows to the row database will take less or more time? Now, let's delete the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('./sql/individual_row_deletes.sql', 'r') as file:\n",
        "    sql_commands = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%timeit -n1 -r1 %sql $sql_commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Row-Based Database: PostgreSQL\n",
        "\n",
        "Up to now, you have run some benchmarking analytical queries with a columnar database. Now, let's perform the same queries with the row database, which is more suitable for transactional operations such as writes and deletes. The PostgreSQL database is loaded with most tables, but due to the main table's size, we have invoked a lambda function to load the main table while you started the lab. You will verify that the table is fully loaded before running the following exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3.1'></a>\n",
        "### 3.1 - Connecting to the Database\n",
        "\n",
        "Let's import the credentials from the environment file and create the connection string to the PostgreSQL database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RDSDBHOST = os.getenv('RDSDBHOST')\n",
        "RDSDBPORT = os.getenv('RDSDBPORT')\n",
        "RDSDBNAME = os.getenv('RDSDBNAME')\n",
        "RDSDBUSER = os.getenv('RDSDBUSER')\n",
        "RDSDBPASSWORD = os.getenv('RDSDBPASSWORD')\n",
        "\n",
        "postgres_connection_url = f'postgresql+psycopg2://{RDSDBUSER}:{RDSDBPASSWORD}@{RDSDBHOST}:{RDSDBPORT}/{RDSDBNAME}'\n",
        "print(postgres_connection_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sql {postgres_connection_url}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test that the main table is fully loaded. Run the following queries, the first will list the available tables in the public schema, the main table is named `lineitem` and should appear. The following query brings the number of items in the `lineitem` table and the count should be `59986052`. If the queries don't return the expected results, wait a few minutes and rerun the queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT * FROM information_schema.tables \n",
        "WHERE table_schema = 'public'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql \n",
        "SELECT count(*) FROM public.lineitem;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 - Executing the Initial Queries\n",
        "\n",
        "Now that you have established the connection to the PostgreSQL database, you will execute the same queries that you created and executed with the Amazon Redshift database in the same order. First, let's start by executing the query that retrieves the information about the orders 1552449, 13620130 and 45619461. You do not have to modify any syntax of your previous query as Amazon Redshift maintains a query syntax compatibility with PostgreSQL. Insert the corresponding query in the cell below, execute it to benchmark the time it takes and compare it with the time it took in Redshift. \n",
        "\n",
        "**Note:** When you execute your query, you will see two connection strings in the output, make sure that it appears as:\n",
        "\n",
        "```bash\n",
        " * postgresql+psycopg2://postgresuser:***@<RDSENDPOINT>.rds.amazonaws.com:5432/dev\n",
        "   redshift+psycopg2://defaultuser:***@<REDSHIFTENDPOINT>.redshift.amazonaws.com:5439/dev\n",
        "```\n",
        "\n",
        "The asterisk must be next to the `postgresql+psycopg2` connection string to use the PostgreSQL connection. Otherwise, you will be using the Redshift connection string; if that's the case, make sure to execute the previous cell that has the command `%sql {postgres_connection_url}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "raw_sql_statement = \"\"\"    \n",
        "    WITH lineitemorders AS (\n",
        "        SELECT *\n",
        "    FROM public.lineitem\n",
        "    WHERE l_orderkey in (1552449, 13620130, 45619461)\n",
        "    )\n",
        "    \n",
        "    SELECT DISTINCT lio.l_orderkey, lio.l_partkey, pt.p_name, ctr.c_custkey, ord.o_orderstatus, ord.o_orderdate, ntn.n_name, rgn.r_name\n",
        "    FROM lineitemorders lio\n",
        "    JOIN part pt ON pt.p_partkey = lio.l_partkey\n",
        "    JOIN orders ord ON lio.l_orderkey = ord.o_orderkey\n",
        "    JOIN customer ctr ON ctr.c_custkey = ord.o_custkey\n",
        "    JOIN nation ntn ON ntn.n_nationkey = ctr.c_nationkey \n",
        "    JOIN region rgn ON rgn.r_regionkey = ntn.n_regionkey\n",
        "    ;\n",
        "\"\"\"\n",
        "\n",
        "sql_statement = format_query(query=raw_sql_statement)\n",
        "\n",
        "%sql {sql_statement}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this query, you're mainly selecting rows from the tables. How does this executing time compare to that of the Redshift database? Now, let's execute the second query which is about extracting the number of customers from the `'MIDDLE EAST'` region:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "raw_sql_statement = \"\"\"    \n",
        "    WITH avg_balance_middle_east AS (\n",
        "        SELECT AVG(c_acctbal) AS avg_balance\n",
        "        FROM customer\n",
        "        JOIN nation ON customer.c_nationkey = nation.n_nationkey\n",
        "        JOIN region ON nation.n_regionkey = region.r_regionkey\n",
        "        WHERE region.r_name = 'MIDDLE EAST'\n",
        "    )\n",
        "    SELECT COUNT(DISTINCT customer.c_custkey)\n",
        "    FROM customer\n",
        "    JOIN nation ON customer.c_nationkey = nation.n_nationkey\n",
        "    JOIN region ON nation.n_regionkey = region.r_regionkey\n",
        "    WHERE region.r_name = 'MIDDLE EAST'\n",
        "      AND customer.c_acctbal > (SELECT avg_balance FROM avg_balance_middle_east);\n",
        "\"\"\"\n",
        "\n",
        "sql_statement = format_query(query=raw_sql_statement)\n",
        "\n",
        "%sql {sql_statement}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, you performed an analytical query that consist of several joins alongside filters and aggregation functions. How much time did it take with Amazon Redshift database?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3.3'></a>\n",
        "### 3.3 - Running the Benchmark Analytical Queries\n",
        "\n",
        "As mentioned earlier in the Amazon Redshift section, you will execute again some TPC-H benchmark queries. Given that you already know them, you will simply execute again each of the following cells. Remember to make a copy of the query in another cell if you want to visualize the results to avoid losing the execution times. In PostgreSQL, some of the benchmark queries can last for more than a minute, it is recommended to wait for the cell to finish before moving to the next one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='TPCHQuery2-01'></a>\n",
        "### First TPC-H Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "\n",
        "sql_statement = format_query(path='./sql/pg_query_0_tcp_h_q14.sql')\n",
        "\n",
        "%sql {sql_statement}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='TPCHQuery2-02'></a>\n",
        "### Second TPC-H Query - Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "\n",
        "sql_statement = format_query(path='./sql/pg_query_1_tcp_h_q6.sql')\n",
        "\n",
        "%sql {sql_statement}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='TPCHQuery2-03'></a>\n",
        "### Third TPC-H Query - Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "\n",
        "sql_statement = format_query(path='./sql/pg_query_2_tcp_h_q18.sql')\n",
        "\n",
        "%sql {sql_statement}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='TPCHQuery2-04'></a>\n",
        "### Fourth TPC-H Query - Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "\n",
        "sql_statement = format_query(path='./sql/pg_query_3_tcp_h_q15.sql')\n",
        "\n",
        "%sql {sql_statement}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='TPCHQuery2-05'></a>\n",
        "### Fifth TPC-H Query - Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "\n",
        "sql_statement = format_query(path='./sql/pg_query_4_tcp_h_q1.sql')\n",
        "\n",
        "%sql {sql_statement}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given that PostgreSQL is a row-based transactional database, the analytical queries take longer to be executed in this system than the Amazon Redshift columnar database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3.4'></a>\n",
        "### 3.4 - Running the Insert and Delete Queries\n",
        "\n",
        "Finally, let's execute the insertion and deletion operations that you performed over the Amazon Redshift database. In this case, given the transactional design of PostgreSQL, you will notice the difference in times when writing and deleting data. Start by inserting the data that you already generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('./sql/individual_row_inserts.sql', 'r') as file:\n",
        "    sql_commands = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%timeit -n1 -r1 %sql $sql_commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How much time did it take compared to Amazon Redshift? You can see that writes in a row-based database are faster in this case. Now let's see how deletions perform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('./sql/individual_row_deletes.sql', 'r') as file:\n",
        "    sql_commands = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%timeit -n1 -r1 %sql $sql_commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, you can see that the write/delete operations are faster in a row database than with a columnar one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Well Done!** In this demonstrative laboratory, you conducted a series of experiments to compare the performance of various queries in Amazon Redshift, a columnar database, and PostgreSQL, a row-oriented database. \n",
        "\n",
        "Amazon Redshift demonstrated a better performance in handling complex aggregation queries. The columnar storage format allowed it to scan only the relevant columns, significantly reducing I/O and speeding up query execution. PostgreSQL, while competent, was generally slower for these types of queries due to its row-oriented storage which necessitates scanning entire rows.\n",
        "\n",
        "PostgreSQL is well-suited for transactional operations and OLTP environments due to its ACID compliance, concurrency control mechanisms, rich data types, and extensibility. While Amazon Redshift excels in analytical workloads, PostgreSQL remains a reliable choice for applications that require robust transaction processing, data integrity, and concurrency management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
