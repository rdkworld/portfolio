{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Lab 2: Streaming Queries with Apache Flink\n",
    "\n",
    "In this lab, you will learn how to perform queries on top of streaming sources using Apache Flink. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Introduction](#1)\n",
    "- [ 2 - Apache Flink 101](#2)\n",
    "  - [ 2.1 - Table Environment](#2.1)\n",
    "  - [ 2.2 - Table Definition](#2.2)\n",
    "  - [ 2.3 - Table Definition with SQL](#2.3)\n",
    "  - [ 2.4 - SQL Queries](#2.4)\n",
    "  - [ 2.5 - Window Queries](#2.5)\n",
    "    - [ Exercise 1](#ex01)\n",
    "    - [ Exercise 2](#ex02)\n",
    "  - [ 2.6 - Output Tables](#2.6)\n",
    "- [ 3 - PyFlink with Kinesis](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from loading the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyflink.table.expressions import col, lit\n",
    "from pyflink.table import EnvironmentSettings, TableEnvironment, DataTypes\n",
    "from pyflink.table.window import Slide, Tumble\n",
    "from pyflink.table.udf import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Introduction\n",
    "\n",
    "In the current landscape of data engineering, one of the biggest challenges is real-time data analysis, which is necessary for certain enterprises that require immediate insights and quick responses to operate their business efficiently. Apache Flink is designed to handle data in real time, allowing you to process and analyze streaming data as soon as it arrives. This enables timely insights and immediate reactions to changing data.\n",
    "\n",
    "Here you will work with a Kinesis data producer similar to the one introduced in Course 1, Week 4. This producer is continuously generating data related to orders made by customers on the [`classicmodels`](https://www.mysqltutorial.org/mysql-sample-database.aspx) website. The business team at Classic Models is eager to gain deeper, real-time insights into the behavior of clients on their websites. By analyzing this streaming data, they aim to understand user buying habits, preferences, and other business metrics as they happen. Through this lab, you will leverage Apache Flink to process and analyze this data stream, providing the business team with valuable, actionable insights in real time.\n",
    "\n",
    "To learn the basics of Apache Flink and PyFlink, you will use a sample dataset based on stock prices. You will follow similar steps with the Classic Models data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Apache Flink 101\n",
    "\n",
    "Apache Flink is a robust framework and processing engine designed for executing computations on data streams. These data streams can be either unbounded, such as continuous streams of real-time data or bounded, like a finite batch of data. Flink is structured around two major APIs: the **DataStream API** and the **Table API & SQL**. The DataStream API is tailored for complex event processing and real-time analytics, while the Table API & SQL offers a more declarative approach to data processing, akin to traditional database operations.\n",
    "\n",
    "In this lab, you will be utilizing PyFlink, which is the Python API for Apache Flink. PyFlink enables developers to harness the power of Flink using Python, making it accessible to those familiar with the language. Additionally, PyFlink integrates with `pandas`, allowing for seamless data manipulation and analysis within the Flink ecosystem. You use the PyFlink Table API to run queries on top of a Kinesis data stream, but first, you will learn how to use PyFlink with some sample data we provided. You can find more information [here](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/table_api_tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 - Table Environment\n",
    "\n",
    "To start using the PyFlink Table API, you first need to declare a table environment. This environment acts as the primary gateway for all interactions with the Flink runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Table Definition\n",
    "\n",
    "The core element of the Table API & SQL in Apache Flink is the entity known as a **Table**. Tables act as both the input and output for queries, serving as the fundamental data structures upon which all operations are performed. In a Flink program, the first step involves defining these tables, meaning defining the schema and the connection details, a table can be connected to various source or target systems, such as databases, files, message queues or data streams.\n",
    "\n",
    "After establishing the tables, you can perform a wide range of operations on them, such as filtering, aggregating, joining, and transforming the data. These operations can be expressed using either the Table API, which provides a programmatic way to manipulate tables, or SQL queries, which offer a more declarative approach. \n",
    "\n",
    "You have been provided two samples of the data produced by the Stock Market and Classic Models Kinesis data streams, located at `data/sample_stock.json` and `data/sample.json` respectively. You will interact with the files using PyFlink before connecting to the Classic Models stream. Load the files using the following cells and understand what each contains: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stock_data = []\n",
    "with open(\"data/sample_stock.json\") as f:\n",
    "    for line in f.readlines():\n",
    "        sample_stock_data.append(json.loads(line))\n",
    "print(json.dumps(sample_stock_data[0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = []\n",
    "with open(\"data/sample.json\") as f:\n",
    "    for line in f.readlines():\n",
    "        sample_data.append(json.loads(line))\n",
    "print(json.dumps(sample_data[0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kinesis Data Streams, and in general streaming sources, are unbounded sequences of records. In the Stock market sample, you have information about the price of a certain ticker at a point in time. Before loading these records to a `Table` in Flink, you will define the schema from source stock data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_stock_schema = DataTypes.ROW([\n",
    "    DataTypes.FIELD(\"event_time\", DataTypes.TIMESTAMP(0)),\n",
    "    DataTypes.FIELD(\"ticker\", DataTypes.STRING()),\n",
    "    DataTypes.FIELD(\"price\", DataTypes.DOUBLE())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, a record for the Classic Models stream seems to be an order made by a customer in JSON format. Using the `DataTypes` classes, define the schema for the sample data in the `row_source_schema` variable.\n",
    "\n",
    "*Note*: DataTypes includes the `INT`, `DATE` and `BOOLEAN` classes, for `TIMESTAMP` we define a value `(p)` where p is the number of digits of fractional seconds (precision), in our case 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_source_schema = DataTypes.ROW([\n",
    "    DataTypes.FIELD(\"order_id\", DataTypes.STRING()),\n",
    "    DataTypes.FIELD(\"order_timestamp\", DataTypes.TIMESTAMP(0)),\n",
    "    DataTypes.FIELD(\"order_date\", DataTypes.DATE()),\n",
    "    DataTypes.FIELD(\"customer_number\", DataTypes.INT()),\n",
    "    DataTypes.FIELD(\"customer_visit_number\", DataTypes.INT()),\n",
    "    DataTypes.FIELD(\"customer_city\", DataTypes.STRING()),\n",
    "    DataTypes.FIELD(\"customer_country\", DataTypes.STRING()),    \n",
    "    DataTypes.FIELD(\"customer_credit_limit\", DataTypes.INT()),\n",
    "    DataTypes.FIELD(\"device_type\",DataTypes.STRING()),\n",
    "    DataTypes.FIELD(\"browser\", DataTypes.STRING()),\n",
    "    DataTypes.FIELD(\"operating_system\", DataTypes.STRING()),\n",
    "    DataTypes.FIELD(\"product_code\",DataTypes.STRING()),\n",
    "    DataTypes.FIELD(\"product_line\", DataTypes.STRING()),\n",
    "    DataTypes.FIELD(\"product_unitary_price\", DataTypes.DOUBLE()),\n",
    "    DataTypes.FIELD(\"in_shopping_cart\", DataTypes.BOOLEAN()),\n",
    "    DataTypes.FIELD(\"quantity\", DataTypes.INT()),\n",
    "    DataTypes.FIELD(\"total_price\", DataTypes.DOUBLE()),\n",
    "    DataTypes.FIELD(\"traffic_source\", DataTypes.STRING())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will enforce this schema on the `sample_stock_data` and `sample_data` arrays, as the JSON files has some of the column values that are meant to be float and date types stored as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in sample_stock_data:\n",
    "    record['event_time'] = datetime.fromisoformat(record['event_time'])\n",
    "    record['price'] = float(record['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same enforcement on the numeric and date values of the `sample_data` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in sample_data:\n",
    "    record['order_timestamp'] = datetime.fromisoformat(record['order_timestamp'])\n",
    "    record['order_date'] = datetime.strptime(record['order_date'],'%Y-%m-%d')\n",
    "    record['product_unitary_price'] = float(record['product_unitary_price'])\n",
    "    record['total_price'] = float(record['total_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can create the `Table` for each sample data using the `from_elements` function, PyFlink has some predefined sources for Python, such as `pandas` dataframe or collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_stock_table = table_env.from_elements(sample_stock_data, row_stock_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_table = table_env.from_elements(sample_data, row_source_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the data is loaded properly, you can convert the PyFlink Tables to Pandas DataFrames and print the first records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_stock_df = source_stock_table.to_pandas()\n",
    "source_stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_table_df = source_table.to_pandas()\n",
    "source_table_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - Table Definition with SQL\n",
    "\n",
    "Now explore an alternative and more intuitive method to define a table in Flink by using SQL and connectors. In this approach, you will define an SQL table that points to the `filesystem` connector and configure various properties such as format and path. Flink provides robust support for connecting to a variety of data sources, including local files, message queues like Kafka, data streams like Kinesis, and even databases. This flexibility allows us to seamlessly integrate Flink with diverse data ecosystems, making it a powerful tool for real-time data processing.\n",
    "\n",
    "You will also add a **WATERMARK** to the timestamp field in the table definition, the watermark helps Flink to recognize the timestamp field as the event time for each record. In Flink, data can be processed based on two types of time columns: processing time and event time. Processing time refers to the system time of the machine running Flink, while event time refers to the actual time at which the events occurred within the stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_table_ddl = \"\"\"\n",
    "  CREATE TABLE stock_data (\n",
    "    event_time TIMESTAMP(0),\n",
    "    ticker STRING,\n",
    "    price NUMERIC,\n",
    "    WATERMARK FOR event_time AS event_time - INTERVAL '5' MINUTES\n",
    "    ) with (\n",
    "        'connector' = 'filesystem',\n",
    "        'format' = 'json',\n",
    "        'path' = '{}',\n",
    "        'json.timestamp-format.standard' = 'ISO-8601'\n",
    "    )\n",
    "\"\"\".format(\"data/sample_stock.json\")\n",
    "table_env.execute_sql(stock_table_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SQL to define the `source_data` table based on the `data/sample.json` file, don't forget to include the WATERMARK for the `order_timestamp` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_env.execute_sql(\"DROP TABLE IF EXISTS source_data\")\n",
    "source_table_ddl = \"\"\"\n",
    "  CREATE TABLE source_data (\n",
    "    order_id STRING,\n",
    "    order_timestamp TIMESTAMP(0),\n",
    "    order_date STRING,\n",
    "    customer_number INT,\n",
    "    customer_visit_number INT,\n",
    "    customer_city STRING,\n",
    "    customer_country STRING,\n",
    "    customer_credit_limit INT,\n",
    "    device_type STRING,\n",
    "    browser STRING,\n",
    "    operating_system STRING,\n",
    "    product_code STRING, \n",
    "    product_line STRING,\n",
    "    product_unitary_price NUMERIC,\n",
    "    quantity INT, \n",
    "    total_price NUMERIC,\n",
    "    traffic_source STRING,\n",
    "    WATERMARK FOR order_timestamp AS order_timestamp - INTERVAL '5' MINUTES\n",
    "    ) with (\n",
    "        'connector' = 'filesystem',\n",
    "        'format' = 'json',\n",
    "        'path' = '{}',\n",
    "        'json.timestamp-format.standard' = 'ISO-8601'\n",
    "    )\n",
    "\"\"\".format(\"data/sample.json\")\n",
    "table_env.execute_sql(source_table_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the from_path function you can bring a table created with SQL as a Table object in Pyflink, we can then run the `execute` and `print` functions to verify that the data was loaded successfully. In the case of the Classic Models data, convert the result to pandas and execute the `head` function, for better readability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_table = table_env.from_path(\"stock_data\")\n",
    "stock_table.execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = table_env.from_path(\"source_data\")\n",
    "input_table_df = input_table.to_pandas()\n",
    "input_table_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "### 2.4 - SQL Queries\n",
    "\n",
    "Now that you have the source data in Table format and in the catalog, you can do run SQL queries on top of the table in the catalog. As the source data is bounded, you can run queries and get results rather quickly. If the source data was in a stream then you would have to find more carefully how your queries are meant to be run, so they don't run indefinitely.\n",
    "\n",
    "Let's start with a simple query. You want to know the average price for the Amazon stock 'AMZN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = table_env.execute_sql(\"\"\"SELECT AVG(price) as avg_price\n",
    "                                     FROM stock_data\n",
    "                                     WHERE ticker = 'AMZN'\n",
    "                                     \"\"\")\n",
    "query_result.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Flink goes through the table, some operations, such as group aggregation, produce update events, and in this case, the aggregation do get updated. Now, create a query to get the total amount that each customer has spend in the Classic Models sample table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = table_env.execute_sql(\"\"\"SELECT customer_number, \n",
    "                                     SUM(total_price) as customer_total_amount\n",
    "                                     FROM source_data GROUP BY customer_number\n",
    "                                     \"\"\")\n",
    "query_result.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.5'></a>\n",
    "### 2.5 - Window Queries\n",
    "\n",
    "Before going into more detail about window queries, let's define a user-defined function (UDF) to convert timestamp fields into string fields. This will help us when writing results into output tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(input_types=[DataTypes.TIMESTAMP(3)], result_type=DataTypes.STRING())\n",
    "def to_string(i):\n",
    "    return str(i)\n",
    "\n",
    "table_env.create_temporary_system_function(\"to_string\", to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windows is the main tool to process unbounded streaming data. It allows us to divide the stream into manageable finite section and appy computations for each section. In Flink, the syntax will be:\n",
    "\n",
    "```python\n",
    "stream.window(...)                 <-  required: \"assigner\"\n",
    "      .reduce/aggregate/apply()    <-  required: \"function\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **tumbling windows assigner** assigns each element to a window of a specified window size, the window size is fixed and the windows don't overlap.\n",
    "\n",
    "![tumbling-windows](images/tumbling-windows.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a Tumble window query, you will look at the minimum price of a stock for 10 second windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tumbling_window_table = (\n",
    "    stock_table.window(\n",
    "        Tumble.over(lit(10).seconds).on(col(\"event_time\")).alias(\"ten_second_window\")\n",
    "    )\n",
    "    .group_by(col('ticker'), col('price'), col('ten_second_window'))\n",
    "    .select(col('ticker'), col('price').min.alias('price'), (to_string(col('ten_second_window').end)).alias('event_time'))\n",
    ")\n",
    "\n",
    "example_tumbling_window_df = example_tumbling_window_table.to_pandas()\n",
    "example_tumbling_window_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex01'></a>\n",
    "### Exercise 1\n",
    "\n",
    "Create a query to get the number of orders in 1-minute windows of time (60 second windows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumbling_window_table = (\n",
    "    input_table.window(\n",
    "        ### START CODE HERE ### (~ 1 line of code)\n",
    "        Tumble.over(lit(60).seconds).on(col(\"order_timestamp\")).alias(\"one_minute_window\") # @REPLACE None.None(None(60).seconds).on(col(\"order_timestamp\")).alias(\"one_minute_window\")\n",
    "        ### END CODE HERE ###\n",
    "    )\n",
    "    .group_by(col('one_minute_window'))\n",
    "    .select((to_string(col('one_minute_window').end)).alias('event_time'),col('order_id').count.distinct.alias('num_orders'))\n",
    ")\n",
    "\n",
    "tumbling_window_df = tumbling_window_table.to_pandas()\n",
    "tumbling_window_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "\n",
    "| **event_time**        | **num_orders** |\n",
    "| --------------------- | -------------- |\n",
    "| 2024-06-01 00:01:00   | 1              |\n",
    "| 2024-06-01 00:02:00   | 2              |\n",
    "| 2024-06-01 00:03:00   | 2              |\n",
    "| 2024-06-01 00:04:00   | 2              |\n",
    "| 2024-06-01 00:05:00   | 2              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: You can also perform window queries with Flink SQL, this is an example of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumble_query = \"\"\"SELECT window_start, window_end, COUNT(DISTINCT order_id) AS total_orders\n",
    "  FROM TABLE(\n",
    "    TUMBLE(TABLE source_data, DESCRIPTOR(order_timestamp), INTERVAL '60' SECONDS))\n",
    "  GROUP BY window_start, window_end; \"\"\"\n",
    "query_result = table_env.execute_sql(tumble_query)\n",
    "query_result.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sliding window assigner** assigns elements to windows of fixed length, it requires two arguments: window size and window slide. The window slide determines how frequently the windows are created, if this value is larger than the window size, then the windows will be overlapping. \n",
    "\n",
    "![sliding-windows](images/sliding-windows.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example on how to use a Sliding window query using PyFlink functions. Here first you will create the sliding window for 10 seconds every 5 seconds on the Stock table, then get the minimum price for each ticker in that window of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sliding_window_table = (\n",
    "    stock_table\n",
    "    .window(\n",
    "        Slide.over(lit(10).seconds)\n",
    "        .every(lit(5).seconds)        \n",
    "        .on(col(\"event_time\"))\n",
    "        .alias(\"ten_second_window\")\n",
    "    )\n",
    "    .group_by(col(\"ticker\"), col(\"ten_second_window\"))\n",
    "    .select(col(\"ticker\"), col(\"price\").min.alias(\"price\"),\n",
    "            to_string(col(\"ten_second_window\").end).alias(\"event_time\"))\n",
    ")\n",
    "\n",
    "example_sliding_window_df = example_sliding_window_table.to_pandas()\n",
    "example_sliding_window_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex02'></a>\n",
    "### Exercise 2\n",
    "\n",
    "Create a sliding window query to get the total amount of sales for a sliding window of 6 minutes, every 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_table = (\n",
    "        input_table.window(\n",
    "            ### START CODE HERE ### (~ 2 lines of code)\n",
    "            Slide.over(lit(6).minute) # @REPLACE None.None(None(6).minute)\n",
    "            .every(lit(3).minutes) # @REPLACE .None(None(3).minutes)\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            .on(col(\"order_timestamp\"))\n",
    "            .alias(\"six_minute_window\")\n",
    "        )\n",
    "        .group_by(col(\"six_minute_window\"))\n",
    "        .select((col(\"six_minute_window\").end).alias(\"event_time\"), col(\"total_price\").sum.alias(\"total_sales\"))\n",
    "    )\n",
    "\n",
    "sliding_window_df = sliding_window_table.to_pandas()\n",
    "sliding_window_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "\n",
    "| **event_time**        | **total_sales** |\n",
    "| --------------------- | --------------- |\n",
    "| 2024-06-01 00:03:00   | 2966            |\n",
    "| 2024-06-01 00:06:00   | 6045            |\n",
    "| 2024-06-01 00:09:00   | 5826            |\n",
    "| 2024-06-01 00:12:00   | 5511            |\n",
    "| 2024-06-01 00:15:00   | 5389            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.6'></a>\n",
    "### 2.6 - Output Tables\n",
    "\n",
    "In Apache Flink, you need to define output tables, known as **sinks**, to store the results of the queries. The process of defining sink tables is similar to defining source tables, with the primary distinction being the specific sink configurations that can be applied. Additionally, Flink provides **print tables**, which are particularly useful for development and debugging purposes. Print tables allow us to output the results of a query directly to the console, enabling us to quickly verify and test our data processing logic. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = table_env.sql_query(\"SELECT customer_number, SUM(total_price) as session_price FROM source_data GROUP BY customer_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_env.create_temporary_view(\"query_result_view\", query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sink_ddl = \"\"\"\n",
    "    create table sink_table (\n",
    "        customer_number INT,\n",
    "        total_price DOUBLE\n",
    "    ) with (\n",
    "        'connector' = 'print'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "table_env.execute_sql(print_sink_ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_result = table_env.execute_sql(\"INSERT INTO {0} SELECT * FROM {1}\"\n",
    "                                         .format(\"sink_table\", \"query_result_view\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can revisit one of the previous example queries. Define a Table with the filesystem connector and save the results of the query into a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sliding_window_table = (\n",
    "    stock_table\n",
    "    .window(\n",
    "        Slide.over(lit(10).seconds)\n",
    "        .every(lit(5).seconds)\n",
    "        .on(col(\"event_time\"))\n",
    "        .alias(\"ten_second_window\")\n",
    "    )\n",
    "    .group_by(col(\"ticker\"), col(\"ten_second_window\"))\n",
    "    .select(col(\"ticker\"), col(\"price\").min.alias(\"price\"),\n",
    "            to_string(col(\"ten_second_window\").end).alias(\"event_time\")))\n",
    "table_env.create_temporary_view(\"example_sliding_window_view\", example_sliding_window_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ddl =  \"\"\" CREATE TABLE json_sink (\n",
    "                ticker VARCHAR(6),\n",
    "                price DOUBLE,\n",
    "                event_time VARCHAR(64)\n",
    "              )\n",
    "              PARTITIONED BY (ticker)\n",
    "              WITH (\n",
    "                'connector' = 'filesystem',\n",
    "                'path' = '{0}',\n",
    "                'format' = 'json',\n",
    "                'json.timestamp-format.standard' = 'ISO-8601'\n",
    "              ) \"\"\".format('data/output_sample')\n",
    "table_env.execute_sql(output_ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_result = table_env.execute_sql(\"INSERT INTO {0} SELECT * FROM {1}\"\n",
    "                                         .format(\"json_sink\", \"example_sliding_window_view\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the folder in the `data/output_sample` path to see the output of the query in JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - PyFlink with Kinesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Flink (and PyFlink) to work with Kinesis, you would need to configure Flink SQL Connectors using JARs. JAR stands for **J**ava **AR**chive, it contains a collection of files that compose a package. Instead of configuring the connectors, to better leverage the power of Kinesis, you are going to use the Amazon Managed Service for Apache Flink service. The Managed service allows to develop Flink applications or to run Flink in notebooks in a Studio environment. You will use it to run queries on the Kinesis Data stream.\n",
    "\n",
    "3.1. To enter the studio which was provided for this lab, return to the AWS console and search for `Managed Apache Flink`. Select the service, and you should see a dashboard like this one:\n",
    "\n",
    "![kinesis dashboard](images/kinesis_dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Select the **Studio notebooks** link in the sidebar of the dashboard, you should see this new Dashboard with a single Studio notebook:\n",
    "\n",
    "![kinesis studio dashboard](images/studio_dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3. Click on the notebook name and click on **Run** button. It takes around 3-5 minutes to get the Studio environment ready. Once it's done, the status should change to `Running`\n",
    "\n",
    "![kinesis studio running](images/studio_running.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4. Select the studio notebook name and then click on **Open in Apache Zeppelin** button. You now should see the Apache Zeppelin Environment UI:\n",
    "\n",
    "![zeppelin dashboard](images/zeppelin_import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5. In Cloud9 or Jupyter environment you should see the `C3_W3_Lab_2_Streaming_Queries_Flink_Studio.zpln` notebook. Download it locally, then go to the Zeppelin UI, click on **Import note**. A new menu should appear. Click on `Select JSON File/IPYNB File` and import the `C3_W3_Lab_2_Streaming_Queries_Flink_Studio.zpln` notebook. The notebook should now appear in the UI:\n",
    "\n",
    "![zeppelin imported](images/importer_notebook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6. Open the notebook and follow the instructions there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
