{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Lab: Simple Data Lake with AWS Glue\n",
    "\n",
    "In this lab, you will work with a simple data lake that uses Amazon S3 as its primary storage. The data lake bucket contains raw JSON files that you will transform using AWS Glue ETL, and then store the processed data in the same bucket. You will finally use AWS Glue crawler to populate the Glue data catalog with metadata about your processed data, and then use Amazon Athena to query your data using SQL statements. In the optional part of the lab, you will explore the effects of compression and partitioning when storing your data in S3.\n",
    "\n",
    "If you get stuck while completing the coding exercises, you can download the solution files by running the following command in your Cloud9 environment:\n",
    "\n",
    "```bash\n",
    "aws s3 cp --recursive s3://dlai-data-engineering/labs/c3w2lab1-706170-solution/ ./\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Introduction](#1)\n",
    "- [ 2 - Raw Data Exploration](#2)\n",
    "  - [ Exercise 1](#ex01)\n",
    "  - [ Exercise 2](#ex02)\n",
    "- [ 3 - Raw Data Processing](#3)\n",
    "  - [ Exercise 3](#ex03)\n",
    "  - [ Exercise 4](#ex04)\n",
    "- [ 4 - Transformation with AWS Glue ETL](#4)\n",
    "  - [ 4.1 - Preparing the Scripts](#4.1)\n",
    "    - [ Exercise 5](#ex05)\n",
    "    - [ Exercise 6](#ex06)\n",
    "  - [ 4.2 - Creating and Running the Glue Jobs](#4.2)\n",
    "- [ 5 - Data Catalog with Glue Crawler](#5)\n",
    "  - [ Exercise 7](#ex07)\n",
    "- [ 6 - Data Querying with Athena](#6)\n",
    "- [ 7 - Optional Experiments: Partitioning and Compression Features of Parquet Format](#7)\n",
    "  - [ 7.1 - Experiments with the Glue Jobs](#7.1)\n",
    "  - [ 7.2 - Experiment Results](#7.2)\n",
    "- [ 8 - Clean up](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import some relevant libraries for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import gzip\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import smart_open\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you work as a Data Engineer at a retailer specializing in scale models of classic cars and other transportation. The data analysts are interested in conducting trend analysis for the top products reviewed in Amazon, to inform new product development. Recently, your team acquired Amazon toy review data and product info, and stored them in a data lake bucket. You are asked to clean the data and ensure its accessibility, so that the data analysts can retrieve the data with SQL-based queries. For the initial testing phase, the team has opted to use AWS Glue ETL for the initial data cleaning, and Amazon Athena to query the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will use [Terraform](https://www.terraform.io/) to define the Glue ETL jobs. You will run these jobs to process the raw datasets and store the results as Parquet files in the same bucket. Using `boto3`, you will create a crawler that you will run over the processed data. The crawler will populate two catalog tables, each linked to a Parquet file. Finally you will query the data using Amazon Athena.\n",
    "\n",
    "<img src=\"images/data_lake.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Raw Data Exploration\n",
    "\n",
    "The dataset consists of two JSON compressed files, one for reviews and one for metadata of the products. Here is an example of one review JSON object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"reviewerID\": \"A2SUAM1J3GNN3B\",\n",
    "  \"asin\": \"0000013714\",\n",
    "  \"reviewerName\": \"J. McDonald\",\n",
    "  \"helpful\": [2, 3],\n",
    "  \"reviewText\": \"I bought this for my husband who plays the piano.  He is having a wonderful time playing these old hymns.  The music  is at times hard to read because we think the book was published for singing from more than playing from.  Great purchase though!\",\n",
    "  \"overall\": 5.0,\n",
    "  \"summary\": \"Heavenly Highway Hymns\",\n",
    "  \"unixReviewTime\": 1252800000,\n",
    "  \"reviewTime\": \"09 13, 2009\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the description of the fields:\n",
    "\n",
    "- `reviewerID` - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- `asin` - ID of the product, e.g. 0000013714\n",
    "- `reviewerName` - name of the reviewer\n",
    "- `helpful` - helpfulness rating of the review, e.g. 2/3\n",
    "- `reviewText` - text of the review\n",
    "- `overall` - rating of the product\n",
    "- `summary` - summary of the review\n",
    "- `unixReviewTime` - time of the review (unix time)\n",
    "- `reviewTime` - time of the review (raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An here's an example of one product metadata JSON object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"asin\": \"0641843224\",\n",
    "  \"description\": \"Set your phasers to stun and prepare for a warp speed ride through the most memorable vocabulary from the sci-fi/fantasy genre.\",\n",
    "  \"title\": \"McNeill Designs YBS Sci-fi/Fantasy Add-on Deck\", \n",
    "  \"price\": 5.19,  \n",
    "  \"imUrl\": \"http://ecx.images-amazon.com/images/I/418t9AN9hiL._SY300_.jpg\", \n",
    "  \"related\": \n",
    "  {\n",
    "    \"also_bought\": [\"B000EVLZ9U\", \"0641843208\", \"0641843216\", \"0641843267\", \"1450751210\", \"0641843232\", \"B00ALQFYGI\", \"B004G7B3NQ\", \"B002PDM288\", \"B009ZNJZV8\", \"B009YG928W\", \"B0063NC3N0\"], \n",
    "    \"also_viewed\": [\"B000EVLZ9U\", \"1450751210\", \"0641843208\", \"0641843267\", \"0641843232\", \"0641843216\", \"B003EIK136\", \"B004G7B3NQ\", \"B003N2Q5JC\"], \n",
    "    \"bought_together\": [\"B000EVLZ9U\"]\n",
    "  },\n",
    "  \"salesRank\": {\"Toys & Games\": 154868}, \n",
    "  \"brand\": \"McNeill Designs\", \n",
    "  \"categories\": [[\"Toys & Games\", \"Games\", \"Card Games\"]]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following fields:\n",
    "\n",
    "- `asin` - ID of the product, e.g. 0000031852\n",
    "- `description` - Description of the product\n",
    "- `title` - name of the product\n",
    "- `price` - price in US dollars (at time of crawl)\n",
    "- `imUrl` - url of the product image\n",
    "- `related` - related products (`also_bought`, `also_viewed`, `bought_together`)\n",
    "- `salesRank` - sales rank information\n",
    "- `brand` - brand name\n",
    "- `categories` - list of categories the product belongs to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the name of the data lake bucket, go to the AWS console and click on the upper right part, where your username appears. Copy the **Account ID**. In the code below, set the variable `BUCKET_NAME` by replacing `<AWS-ACCOUNT-ID>` placeholder with the Account ID that you copied. The Account ID should contain only numbers without hyphens between them (e.g. 123412341234, not 1234-1234-1234 and the bucket name should have the same format as `de-c3w2lab1-123412341234-us-east-1-data-lake`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'de-c3w2lab1-<AWS-ACCOUNT-ID>-us-east-1-data-lake'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data lake bucket contains the raw JSON files. The bucket also contains samples of each dataset, which are of smaller size of the original data, that you will next interact with to explore the content of the data. The next cell consists of a function that you will use to load the data samples into a Pandas DataFrame so you can explore them. Run the following cell to define the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_sample(bucket_name: str, s3_file_key: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads review sample dataset\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): Bucket name\n",
    "        s3_file_key (str): Dataset s3 key location\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Read dataframe\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    source_uri = f's3://{bucket_name}/{s3_file_key}'\n",
    "    json_list = []\n",
    "    for json_line in smart_open.open(source_uri, transport_params={'client': s3_client}):\n",
    "        json_list.append(json.loads(json_line))\n",
    "    df = pd.DataFrame(json_list)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex01'></a>\n",
    "### Exercise 1\n",
    "\n",
    "Complete the code to call the `read_data_sample()` function passing the `BUCKET_NAME` and the file key parameter as `'staging/reviews_Toys_and_Games_sample.json.gz'`. Then, take a look at the first 5 rows of this sample reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (1 line of code)\n",
    "review_sample_df = read_data_sample(bucket_name=BUCKET_NAME, s3_file_key='staging/reviews_Toys_and_Games_sample.json.gz') # @REPLACE EQUALS None(bucket_name=None, s3_file_key='None')\n",
    "### END CODE HERE ###\n",
    "\n",
    "review_sample_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data type of each of the columns of the dataset, and pay closer attention to the `unixReviewTime` and `helpful` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the `unixReviewTime` value is currently an integer that represents a Unix timestamp defined in terms of the number of seconds since January 1st, 1970 at UTC. Each `helpful` column consists of two numbers: the number of users who found the review helpful and the total number of users who rated the helpfulness of this review. Later in Exercise 3, you will transform this raw data to make it more useful for further analysis.\n",
    "\n",
    "Let's check out the metadata dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex02'></a>\n",
    "### Exercise 2\n",
    "\n",
    "Execute the `read_data_sample()` function with the same bucket name, but now set the file key parameter as `'staging/meta_Toys_and_Games_sample.json.gz'`. Then take a look at the first 5 rows of this sample metadata dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (1 line of code)\n",
    "metadata_sample_df = read_data_sample(bucket_name=BUCKET_NAME, s3_file_key='staging/meta_Toys_and_Games_sample.json.gz') # @REPLACE EQUALS None(bucket_name=None, s3_file_key='None')\n",
    "### END CODE HERE ###\n",
    "\n",
    "metadata_sample_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, take a look at the column's data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_sample_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this data more useful for analysis, later in Exercise 4 you will perform some transformations to the data. For example, you will extract the sales rank and category from the `salesRank` column and save them into two separate columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Raw Data Processing\n",
    "\n",
    "In this section, you will complete two processing functions that you will later incorporate into the Glue job scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex03'></a>\n",
    "### Exercise 3\n",
    "\n",
    "In this exercise, you will perform some transformations on the reviews dataset to make the data from the `unixReviewTime` and `helpful` columns more useful for analysis. \n",
    "\n",
    "Complete the `process_review()` function with the following transformations:\n",
    "\n",
    "1. Convert the `unixReviewTime` column to date with the `pd.to_datetime()` function. Remember that this timestamp is defined in seconds (use `s` for the `unit` parameter). Save the result in the column `reviewTime`.\n",
    "2. Extract the year and month from the `reviewTime`, and save those values in new columns named `year` and `month`. You can apply `.dt.year` and `.dt.month` methods to `raw_df['reviewTime']` to do that. You will later use these columns to partition the processed data in the `S3` bucket.\n",
    "3. Create a new DataFrame named `df_helpful` based on converting the `helpful` column from the `raw_df` into a list with the `to_list()` method. Set the column names as `helpful` and `totalHelpful`. \n",
    "4. With the `pd.concat()` function, concatenate the `raw_df` dataframe with `df_helpful`. From the `raw_df` drop the `helpful` column using the `raw_df.drop()` function, set `axis=1`.\n",
    "\n",
    "Then, perform the transformations and check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review(raw_df: pd.DataFrame) -> pd.DataFrame:    \n",
    "    \"\"\"Transformations steps for Reviews dataset\n",
    "\n",
    "    Args:\n",
    "        raw_df (DataFrame): Raw data loaded in dataframe\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Returned transformed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (5 lines of code)\n",
    "    raw_df['reviewTime'] = pd.to_datetime(raw_df['unixReviewTime'], unit='s') # @REPLACE EQUALS pd.None(None['None'], unit='None')\n",
    "    raw_df['year'] = raw_df['reviewTime'].dt.year # @REPLACE EQUALS None['None'].None.None\n",
    "    raw_df['month'] = raw_df['reviewTime'].dt.month # @REPLACE EQUALS None['None'].None.None\n",
    "    \n",
    "    df_helpful = pd.DataFrame(raw_df['helpful'].to_list(), columns=['helpful', 'totalHelpful']) # @REPLACE EQUALS pd.DataFrame(None['None'].None(), columns=['None', 'None'])\n",
    "    target_df = pd.concat([raw_df.drop(columns=['helpful']), df_helpful], axis=1) # @REPLACE EQUALS None.None([None.None(columns=['None']), None], axis=1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return target_df\n",
    "\n",
    "transformed_review_sample_df = process_review(raw_df=review_sample_df)\n",
    "transformed_review_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex04'></a>\n",
    "### Exercise 4\n",
    "\n",
    "In this exercise, you will perform some transformations on the reviews metadata dataset to make the data from the `salesRank` column more useful for analysis. You will also remove some of the null values and replace others with an empty string.\n",
    "\n",
    "Complete the function provided in the next cell with the following steps: \n",
    "\n",
    "1. Remove any records that have null values for the `salesRank` column. You can use the `dropna()` method with the parameter `how=\"any\"`. Save the resulting dataframe in the `tmp_df` variable.\n",
    "2. Create a dataframe named `df_rank`. This dataframe should contain two columns named `'sales_category'` and `'sales_rank'` that come from extracting the key and value respectively from the `'salesRank'` column.\n",
    "3. Concatenate the `tmp_df` and `df_rank` dataframes and save the result in a new dataframe named `target_df`. You can use function `pd.concat()` to do that.\n",
    "4. Use the parameter `cols` to select the desired columns in the dataframe `target_df`.\n",
    "5. From `target_df`, remove any records that have null values for the `asin`, `price` and `sales_rank` columns. Again, use the `dropna()` method with the parameter `how=\"any\"`.\n",
    "6. Fill the null value of the rest of the dataframe with an empty string `\"\"`. You can use `fillna()` function to do that.\n",
    "\n",
    "Then, perform the transformations and check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metadata(raw_df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Function in charge of the transformation of the raw data of the\n",
    "    Reviews Metadata.\n",
    "\n",
    "    Args:\n",
    "        raw_df (DataFrame): Raw data loaded in dataframe\n",
    "        cols (list): List of columns to select\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Returned transformed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (6 lines of code)\n",
    "    tmp_df = raw_df.dropna(subset=[\"salesRank\"], how=\"any\") # @REPLACE EQUALS None.None(subset=[\"None\"], how=\"None\")\n",
    "    \n",
    "    df_rank = pd.DataFrame([{\"sales_category\": key, \"sales_rank\": value} for d in tmp_df[\"salesRank\"].tolist() for key, value in d.items()]) # @REPLACE EQUALS pd.DataFrame([{\"None\": key, \"None\": value} for d in tmp_df[\"salesRank\"].tolist() for key, value in d.items()])\n",
    "    \n",
    "    target_df = pd.concat([tmp_df, df_rank], axis=1) # @REPLACE EQUALS None.None([None, None], axis=1)\n",
    "    target_df = target_df[cols] # @REPLACE EQUALS target_df[None]\n",
    "    target_df = target_df.dropna(subset=[\"asin\", \"price\", \"sales_rank\"], how=\"any\") # @REPLACE EQUALS target_df.None(subset=[\"None\", \"None\", \"None\"], how=\"None\")\n",
    "    target_df = target_df.fillna(\"\") # @REPLACE EQUALS target_df.None(\"\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return target_df\n",
    "\n",
    "processed_metadata_df = process_metadata(raw_df=metadata_sample_df, \n",
    "                                         cols=['asin', 'description', 'title', 'price', 'brand','sales_category','sales_rank']\n",
    "                                         )\n",
    "processed_metadata_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have explored and performed some transformations on the reviews and metadata sample datasets, and you are now ready to perform those transformations over the complete dataset. This process will be done with AWS Glue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Transformation with AWS Glue ETL\n",
    "\n",
    "AWS Glue ETL (Extract, Transform, Load) is a serverless service that simplifies data integration and processing. You briefly used Glue in the labs of course 1. In this lab, you will interact more closely with this service, and in course 4, you will learn more details about its underlying distributed framework (Apache Spark). \n",
    "\n",
    "AWS Glue requires a Spark script to perform a job, this script can be coded in Python or Scala. Here you are provided with Python scripts. In these scripts, you will extract the raw data from the provided bucket, transform the data and then save it in the **parquet** format. Parquet is a columnar storage file format commonly used in big data processing frameworks like Apache Hadoop and Apache Spark. Parquet format has several features such as the support of compression algorithms and support of schema evolution. If you want to know more about this format, you can read [this article](https://airbyte.com/data-engineering-resources/parquet-data-format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.1'></a>\n",
    "### 4.1 - Preparing the Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex05'></a>\n",
    "### Exercise 5\n",
    "\n",
    "You will now complete the script to transform the Amazon Reviews.\n",
    "\n",
    "1. Open the file `terraform/assets/de-c3w2-reviews-transform-job.py`. The script reads the data from the JSON file, performs basic transformations, and saves the result in a parquet file.\n",
    "\n",
    "2. Before completing the incomplete `transform()` function, read through the entire code and comments to get an overview of how the three steps - Extract, Transform, and Load - are implemented.\n",
    "\n",
    "3. Complete the `transform()` function by copying part of the code you completed in [Exercise 3](#ex03).\n",
    "\n",
    "4. Save changes to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex06'></a>\n",
    "### Exercise 6\n",
    "\n",
    "You will now complete the script to transform the Amazon Product Metadata.\n",
    "\n",
    "1. Open the file `terraform/assets/de-c3w2-metadata-transform-job.py`.\n",
    "\n",
    "2. Before completing the incomplete `transform()` function, read through the entire code and comments to get an overview of how the three steps - Extract, Transform, and Load - are implemented. \n",
    "\n",
    "3. Complete the `transform()` function by copying part of the code that you completed in [Exercise 4](#ex04). \n",
    "\n",
    "4. Save changes to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.2'></a>\n",
    "### 4.2 - Creating and Running the Glue Jobs\n",
    "\n",
    "After completing the scripts, you will now create the resources needed to run the Glue jobs using Terraform. Once you create the resources, you will run the AWS Glue Jobs to ingest the raw data from the source bucket and transform it. You are provided with the Terraform configuration files to create the Glue jobs, your task is to deploy the jobs. For a detailed overview of the provided terraform files, make sure to watch the lab walkthrough video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1. Open the `./terraform/glue.tf` file. You will see two different Glue jobs: one for the reviews and another for the product metadata. For now, leave the file as is. Note the use of the `Snappy` compression algorithm for both jobs and the choice of partitioning columns: `year` and `month` for the reviews, and `sales_category` for the product metadata. In the optional part of the lab at the end, you will make changes to the compression algorithm and partitioning columns to perform various experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2. In the Cloud9 or Jupyter terminal, navigate to the `terraform` folder, initialize Terraform and then deploy the resources by running the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd terraform\n",
    "terraform init\n",
    "terraform plan\n",
    "terraform apply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: After the `terraform apply` command, you will need to type `yes` and press `Enter` to confirm that you want Terraform to apply the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will include the name of each of the glue jobs and the IAM role used, as follows:\n",
    "\n",
    "```bash\n",
    "glue_role = \"Cloud9-de-c3w2lab1-glue-role\"\n",
    "metadata_glue_job = \"de-c3w2lab1-metadata-etl-job\"\n",
    "reviews_glue_job = \"de-c3w2lab1-reviews-etl-job\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These outputs will be used later in the lab, you can save them locally. Note that you will only need the values of the job names (i.e., `de-c3w2lab1-metadata-etl-job` and `de-c3w2lab1-reviews-etl-job`) for the next step. The key names (`metadata_glue_job` and `reviews_glue_job`) are provided for reference but are not used in the commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3. Run each job with the command below, replacing the placeholder `<GLUE-JOB-NAME>` with the job name from the Terraform output (`de-c3w2lab1-reviews-etl-job` or `de-c3w2lab1-metadata-etl-job`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "aws glue start-job-run --job-name <GLUE-JOB-NAME> | jq -r '.JobRunId' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "4.2.4. You can check the status of each Glue job in the console, or from the terminal by running the following command. Make sure to exchange the `<GLUE-JOB-NAME>` with the job name, and `<JOB-RUN-ID>` with the output from the previous step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "```bash\n",
    "aws glue get-job-run --job-name <GLUE-JOB-NAME> --run-id <JOB-RUN-ID> --output text --query \"JobRun.JobRunState\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "When each job has a `SUCCEEDED` status, you can continue with the rest of the lab. Take into account that each run of `de-c3w2lab1-metadata-etl-job` can take around 3 minutes while each run of `de-c3w2lab1-reviews-etl-job` can take between 7 to 8 minutes.\n",
    "- The processed metadata will be stored at: `s3://<BUCKET_NAME>/processed_data/snappy/partition_by_sales_category/toys_metadata/`.\n",
    "- The processed reviews will be stored at:\n",
    "`s3://<BUCKET_NAME>/processed_data/snappy/partition_by_year_month/toys_reviews/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Data Catalog with Glue Crawler\n",
    "\n",
    "AWS Glue Crawler is a powerful automated tool offered by AWS Glue for discovering and cataloging metadata about data sources, which enables services like Glue and Athena to query data directly from different sources. By simply pointing the crawler to your data source, whether it's a database or a data lake, it will automatically scan and extract the schema information, data types, and other relevant metadata. This metadata is then organized and stored in a database table in the AWS Glue Data Catalog, providing a centralized repository for understanding and managing your data assets. In this lab, you will create a crawler that will be in charge of populating the Glue Data Catalog with the newly transformed data in S3, you will be able to query the data directly from S3 using Athena in the final part.\n",
    "\n",
    "Start by checking the databases in the data catalog using AWS SDK for pandas (`awswrangler`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = wr.catalog.databases()\n",
    "print(databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data catalog is empty, let's create a new catalog database labeled as `de-c3w2lab1-aws-reviews` in which the crawler will store the metadata of the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_NAME = \"de-c3w2lab1-aws-reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATABASE_NAME not in databases.values:\n",
    "    wr.catalog.create_database(DATABASE_NAME)\n",
    "    print(wr.catalog.databases())\n",
    "else:\n",
    "    print(f\"Database {DATABASE_NAME} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex07'></a>\n",
    "### Exercise 7\n",
    "\n",
    "Complete the code to create a Glue crawler utilizing the `boto3` library:\n",
    "\n",
    "1. Define the Role parameter with the Glue Job role that you obtained from the `terraform` output (step 4.2.2).\n",
    "2. Add a description to the crawler (e.g. 'Amazon Reviews for toys').\n",
    "3. Define the S3 targets with the  paths in your data lake bucket for the `toys_reviews` table (partitioned by year and month and with `snappy` compression) and the `toys_metadata` table (partitioned by sales category with `snappy` compression). You just need to replace `None` with the `BUCKET_NAME` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue',region_name=\"us-east-1\")\n",
    "configuration= {\"Version\": 1.0,\"Grouping\": {\"TableGroupingPolicy\": \"CombineCompatibleSchemas\" }}\n",
    "\n",
    "response = glue_client.create_crawler(\n",
    "    Name='de-c3w2lab1-crawler',\n",
    "    ### START CODE HERE ### (12 lines of code)\n",
    "    Role='Cloud9-de-c3w2lab1-glue-role', # @REPLACE EQUALS 'None',\n",
    "    DatabaseName=DATABASE_NAME, # @KEEP\n",
    "    Description='Amazon Reviews for toys', # @REPLACE EQUALS 'None',\n",
    "    Targets={ # @KEEP\n",
    "        'S3Targets': [ # @KEEP\n",
    "            { # @KEEP\n",
    "                'Path': f's3://{BUCKET_NAME}/processed_data/snappy/partition_by_year_month/toys_reviews/', # @REPLACE 'Path': f's3://{None}/processed_data/snappy/partition_by_year_month/toys_reviews/',\n",
    "            },# @KEEP\n",
    "            { # @KEEP\n",
    "                'Path': f's3://{BUCKET_NAME}/processed_data/snappy/partition_by_sales_category/toys_metadata/', # @REPLACE 'Path': f's3://{None}/processed_data/snappy/partition_by_sales_category/toys_metadata/',\n",
    "            } # @KEEP            \n",
    "        ]} # @KEEP\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the crawler was indeed created, using the `list_crawlers` method of the `boto3` Glue Client. If the creation was successful, you should see the new crawler in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.list_crawlers()\n",
    "print(response['CrawlerNames'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the crawler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.start_crawler(\n",
    "    Name='de-c3w2lab1-crawler'\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crawler will start looking for data sources in the S3 targets that we have defined, it should take around **3 minutes** for the first run. \n",
    "\n",
    "After around 3 minutes, check that the two tables were created for each processed dataset using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.catalog.tables(database=DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to the last section of the lab where you will use Amazon Athena, make sure that two catalog tables were created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Data Querying with Athena\n",
    "\n",
    "Amazon Athena is a serverless, interactive query service provided by AWS, allowing you to analyze data in Amazon S3 using standard SQL. It enables you to quickly and easily query vast amounts of data stored in various formats such as CSV, JSON, Parquet, and more, without needing to set up and manage complex infrastructure. Athena leverages the AWS Glue Data Catalog to access the stored metadata associated with the S3 data. By utilizing this metadata, Athena seamlessly executes queries on the underlying data, streamlining the analytical process and enabling efficient data exploration and analysis.\n",
    "\n",
    "You will now test some of the queries that the data analyst will run. You will use the `awswrangler` library to run these queries.\n",
    "\n",
    "*Note*: `awswrangler` uses the `pyarrow` library that has some functions that returns a `FutureWarning` due to deprecation, this doesn't affect what you are trying to do so you will filter out those warnings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this test query to retrieve a sample of 5 records from the toys_reviews table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT * FROM toys_reviews LIMIT 5\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run this test query to find the top 5 products with the most reviews, return the name of the product and the count of reviews. Ignore the products with an empty title in the metadata table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT met.title, count(distinct toy.reviewerid) as review_count\n",
    "FROM toys_metadata met \n",
    "LEFT JOIN toys_reviews toy\n",
    "ON met.asin = toy.asin\n",
    "WHERE met.title <> ''\n",
    "GROUP BY met.title\n",
    "ORDER BY count(distinct toy.reviewerid) DESC\n",
    "LIMIT 5\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next test query to find the top 10 products in terms of average rating, but the products should have at least 1000 reviews. Return the title, sales category and the average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT met.title, met.sales_category, avg(toy.overall) as review_avg\n",
    "FROM toys_metadata met \n",
    "LEFT JOIN toys_reviews toy\n",
    "ON met.asin = toy.asin\n",
    "GROUP BY met.title, met.sales_category\n",
    "HAVING count(distinct toy.reviewerid) > 1000\n",
    "ORDER BY avg(toy.overall) DESC\n",
    "LIMIT 10\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run this query to determine the average rating for each brand and the number of products they have in the database. Only show the top 10 brands with the highest product counts in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT met.brand, count(distinct met.asin) as product_count, avg(toy.overall) as review_avg\n",
    "FROM toys_metadata met \n",
    "LEFT JOIN toys_reviews toy\n",
    "ON met.asin = toy.asin\n",
    "WHERE met.brand <> ''\n",
    "GROUP BY met.brand\n",
    "ORDER BY count(distinct toy.asin) DESC\n",
    "LIMIT 10\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last test query, you want to look at 25 random reviews that gave a rating of 5 to a toy car product. Return the product title, description, the review text and overall score. Ignore products that have an empty title, use the LIKE operator to search reviews with toy car in their text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT met.title, met.description, toy.reviewtext, toy.overall\n",
    "FROM toys_reviews toy\n",
    "LEFT JOIN toys_metadata met\n",
    "ON toy.asin = met.asin\n",
    "WHERE toy.reviewtext like '%toy car%' and toy.overall = 5.0 and met.title <> '' \n",
    "LIMIT 25\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "The next section is optional. If you'd like to try it, first ensure you submit the lab for grading, and then proceed with the experiments. Otherwise, feel free to skip to [the last section](#8). For a summary of the optional section, you can watch the lab walkthrough video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Optional Experiments: Partitioning and Compression Features of Parquet Format\n",
    "\n",
    "**Compression**\n",
    "\n",
    "The Parquet format, commonly used in data lake architectures, supports various [compression codecs](https://parquet.apache.org/docs/file-format/data-pages/compression/) such as Snappy, Gzip, and LZO. These codecs play a crucial role in reducing the storage space needed by Parquet files. However, they come with a trade-off: while they reduce storage costs and optimize resource utilization, they can also impact processing speeds during data ingestion, transformation, and querying. To test this with a practical example, you will store the same transformed data using three different compression options: `UNCOMPRESSED`, `SNAPPY` and `GZIP`: \n",
    "\n",
    "- `UNCOMPRESSED`: Data is left uncompressed.\n",
    "- `SNAPPY`: Snappy is a fast compression/decompression library that works well with Parquet files. It provides good compression ratios and is optimized for speed. Snappy compression is often a good choice for balancing compression efficiency and query performance.\n",
    "- `GZIP`: Gzip is a widely used compression algorithm that provides high compression ratios. However, it tends to be slower in both compression and decompression compared to Snappy. Gzip compression can achieve higher levels of compression but may result in slower query performance.\n",
    "\n",
    "**Partitioning**\n",
    "\n",
    "Partitioning in Parquet is a technique used to better organize data within Parquet files based on partition keys. By partitioning data, Parquet optimizes query performance by reducing the amount of data that needs to be scanned during query execution. \n",
    "\n",
    "**Experiments**\n",
    "\n",
    "To explore the compression features, you will process the product metadata 3 times; in each time, you will choose a different option for the compression algorithm with no partitioning as shown in the left table here. Then you will compare for this data, \"uncompressed versus Snappy\" and then \"Snappy versus Gzip\". \n",
    "\n",
    "To explore the partitioning features, you will process the review dataset 3 times; in each time, you will choose a different option for the partitioning column, with Snappy for compression. Then you will compare for this data, \"partitioning versus no partitioning\" and then \"partitioning by year and month\" versus \"partitioning by the product id (asin)\".\n",
    "\n",
    "<img src=\"images/experiment.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7.1'></a>\n",
    "### 7.1 - Experiments with the Glue Jobs\n",
    "\n",
    "Open the file `./terraform/glue.tf` where you will be specifying the parameters for each of the experiments. Remember to save changes by pressing `Ctrl+S` or `Cmd+S` before redeploying each job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp01'></a>\n",
    "### Metadata - No Partitioning, Uncompressed\n",
    "\n",
    "Search for the `resource \"aws_glue_job\" \"metadata_etl_job\"` terraform resource. In the `default_arguments` parameter, set the following:\n",
    "- `--compression` to `\"uncompressed\"` indicating that the parquet files will be saved without any compression,\n",
    "- `--partition_cols` to `jsonencode([])`, which will be an empty list in the partition columns, no partitioning will be performed. \n",
    "\n",
    "Save the changes and follow steps 4.2.2 - 4.2.4. Run only the metadata Glue job. The results of this experiment will be stored at `s3://<BUCKET_NAME>/processed_data/uncompressed/no_partition/toys_metadata/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp02'></a>\n",
    "### Metadata - No Partitioning, Snappy\n",
    "\n",
    "Search for the `resource \"aws_glue_job\" \"metadata_etl_job\"` terraform resource. In the `default_arguments` parameter, set \n",
    "- `--compression` to `\"snappy\"` indicating that the parquet files will be compressed using the `SNAPPY` algorithm,\n",
    "- `--partition_cols` to `jsonencode([])`. \n",
    "\n",
    "Again, save the changes and follow steps 4.2.2 - 4.2.4. Run only the metadata Glue job. The results of this experiment will be stored at `s3://<BUCKET_NAME>/processed_data/snappy/no_partition/toys_metadata/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp03'></a>\n",
    "### Metadata - No partitioning, Gzip\n",
    "\n",
    "Search for the `resource \"aws_glue_job\" \"metadata_etl_job\"` terraform resource. In the `default_arguments` parameter, set \n",
    "- `--compression` to `\"gzip\"` indicating that the parquet files will be compressed using the `GZIP` algorithm,\n",
    "- `--partition_cols` to `jsonencode([])`. \n",
    "\n",
    "Again, save the changes and follow steps 4.2.2 - 4.2.4. Run only the metadata Glue job. The results of this experiment will be stored at `s3://<BUCKET_NAME>/processed_data/gzip/no_partition/toys_metadata/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp04'></a>\n",
    "### Reviews - No partitioning, Snappy\n",
    "\n",
    "Search for the `resource \"aws_glue_job\" \"reviews_etl_job\"` terraform resource. In the `default_arguments` parameter, set \n",
    "- `--compression` to `\"snappy\"`; this will indicate the job that the parquet files will be compressed using the `SNAPPY` algorithm,\n",
    "- `--partition_cols` to `jsonencode([])`.\n",
    "\n",
    "Run only the review Glue job. After running the Glue Job, the results of this experiment will be stored at `s3://<BUCKET_NAME>/processed_data/snappy/no_partition/toys_reviews/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp05'></a>\n",
    "### Reviews - Partitioning by year and month, Snappy\n",
    "\n",
    "Here you don't need to run any glue jobs. You've already run this Glue job with Snappy as the compression algorithm and using the year and month columns as the partitioning key. The results are stored at `s3://<BUCKET_NAME>/processed_data/snappy/partition_by_year_month/toys_reviews/`.\n",
    "The parquet files are partitioned by the value of the two columns, year and month. The first partition is done by `year`and then, each `year` is partitioned into sub-partitions according to the `month`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp06'></a>\n",
    "### Reviews - Partitioning by asin, Snappy\n",
    "\n",
    "Search for the `resource \"aws_glue_job\" \"reviews_etl_job\"` terraform resource. In the `default_arguments` parameter, set \n",
    "- `--compression` to `\"snappy\"`; \n",
    "- `--partition_cols` to `jsonencode([\"asin\"])`. \n",
    "\n",
    "Create and run only the review Glue Job. Take into account that this experiment is expected to fail due to a **timeout** (It will run for 15 minutes and then stop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7.2'></a>\n",
    "### 7.2 - Experiment Results\n",
    "\n",
    "Now that your jobs have succeeded, and the processed data of each experiment has been stored in its corresponding locations, in this section you'll analyze the results.\n",
    "\n",
    "**Compressed vs Uncompressed data**\n",
    "\n",
    "First, let's compare between uncompressed and compressed data with no partitions using Metadata. Run the following commands to take a look to the size of the stored files. Remember that you can also run these commands in the Cloud9 terminal.\n",
    "\n",
    "Read the file sizes of Metadata - No partitioning, Uncompressed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/uncompressed/no_partition/toys_metadata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file sizes of Metadata - No partitioning, Snappy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/no_partition/toys_metadata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that uncompressed files are bigger in size than the compressed ones, by almost a factor of 2. Also, even though you didn't specify a partition key, there are several files instead of only one. Some processing frameworks such as Apache Spark and AWS Glue will by default try to partition your data automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Snappy vs Gzip**\n",
    "\n",
    "Now, let's take a look at the results from applying two different compression algorithms.Run the following commands.\n",
    "\n",
    "Read the file sizes of Metadata - No partitioning, Snappy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/no_partition/toys_metadata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file sizes of Metadata - No partitioning, Gzip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/gzip/no_partition/toys_metadata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the compression algorithms, you can see how `GZIP` has a better compression rate than `SNAPPY`. However, this higher compression rate comes at the cost of being less efficient at querying the data when using `GZIP`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partition keys**\n",
    "\n",
    "Finally, let's compare the usage of partition keys with the reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No partition keys vs using partition keys**- Let's look again at the results of Reviews - Partitioning by year and month versus no_partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/no_partition/toys_reviews/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/partition_by_year_month/toys_reviews/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the partitioned results, you will find a bunch of folders with a structure similar this:\n",
    "\n",
    "```bash\n",
    ".  \n",
    " processed_data/\n",
    "     snappy/\n",
    "         partition_by_year_month/\n",
    "             toys_reviews/\n",
    "                 year = <value-1>/\n",
    "                |    month = <value-x>\n",
    "                |   |     run-<TIMESTAMP>-part-block-0-0-r-00000-snappy.parquet\n",
    "                |   |     run-<TIMESTAMP>-part-block-0-0-r-00001-snappy.parquet\n",
    "                |   |    ...\n",
    "                |   |     run-<TIMESTAMP>-part-block-0-0-r-<PARTITION-ID>-snappy.parquet\n",
    "                |   ...\n",
    "                |    month = <value-z>\n",
    "                |         run-<TIMESTAMP>-part-block-0-0-r-00000-snappy.parquet\n",
    "                |         run-<TIMESTAMP>-part-block-0-0-r-00001-snappy.parquet\n",
    "                |        ...\n",
    "                |         run-<TIMESTAMP>-part-block-0-0-r-<PARTITION-ID>-snappy.parquet\n",
    "                ...                \n",
    "                 year = <value-n>/\n",
    "                     month = <value-x>\n",
    "                    |     run-<TIMESTAMP>-part-block-0-0-r-00000-snappy.parquet\n",
    "                    |     run-<TIMESTAMP>-part-block-0-0-r-00001-snappy.parquet\n",
    "                    |    ...\n",
    "                    |     run-<TIMESTAMP>-part-block-0-0-r-<PARTITION-ID>-snappy.parquet\n",
    "                    ...\n",
    "                     month = <value-z>\n",
    "                          run-<TIMESTAMP>-part-block-0-0-r-00000-snappy.parquet\n",
    "                          run-<TIMESTAMP>-part-block-0-0-r-00001-snappy.parquet\n",
    "                         ...\n",
    "                          run-<TIMESTAMP>-part-block-0-0-r-<PARTITION-ID>-snappy.parquet\n",
    "```\n",
    "\n",
    "The folders you see have names made up of the partition key (or column name) and the corresponding partition value. Inside each folder, you'll find one or more parquet files with particular IDs. You can see that the total size is quite similar. \n",
    "\n",
    "Partition keys must be set in a way that distributes the data as evenly as possible across the partitions, as you will see in the next experiment.\n",
    "\n",
    "**Setting appropriate partition keys**:\n",
    "Now, let's take a look at the results of the last Glue job. The job will run for 15 minutes, and then you will get a timeout error if you check the status of the glue job in the console.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/partition_by_year_month/toys_reviews/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The execution of this cell may take around 1-2 minutes:\n",
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/partition_by_asin/toys_reviews/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the final output of these last two commands, you can see that using the year and month columns as partition keys generated around 556 files with a total size of less than 600MB. On the other hand, using the asin column (which is the identifier for each product) as the partition key can generate thousands of files. However, this job failed due to a timeout, resulting in a smaller total size as not all data was saved. This illustrates one of the issues that may arise when using inappropriate partition keys.\n",
    "\n",
    "With year and month as partition keys, the number of reviews can be more evenly distributed across dates, resulting in a manageable number of files with sizes which can be later easily processed. However, with the asin partition key, given that some products may have very few reviews and there's a large number of products, the Glue job couldn't save all files within the allotted time.\n",
    "\n",
    "Having too many small files can have several disadvantages:\n",
    "\n",
    "* Increased Metadata Overhead: Each Parquet file carries metadata. With numerous small files, managing this metadata can significantly impact performance.\n",
    "* Higher Storage Costs: Storing many small files can result in higher costs compared to fewer larger files, as cloud storage providers often charge based on the number of objects stored.\n",
    "* Slower Filesystem Operations: Operations like listing or opening files may become slower with numerous small files, affecting performance of administrative tasks or data processing.\n",
    "* Suboptimal Data Processing: Some data processing frameworks may be less efficient when handling many small files, incurring overhead in file opening, network communication, and task scheduling.\n",
    "\n",
    "The key is to balance partitioning data for query performance while avoiding the drawbacks of too many small files. It's recommended to choose a partitioning strategy that aligns with your querying patterns and dataset scale, considering factors like storage costs, filesystem limitations, and processing efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "## 8 - Clean up\n",
    "\n",
    "After finishing the experiments and exercises, you will have to delete some of the created resources, let's start with the resources created with terraform. Run the following command inside the `terraform` folder:\n",
    "\n",
    "```bash\n",
    "terraform destroy\n",
    "```\n",
    "\n",
    "Now, let's delete the AWS Glue crawler and database, run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = glue_client.delete_crawler(\n",
    "        Name='de-c3w2lab1-crawler'\n",
    "    )\n",
    "    print(\"Crawler deleted successfully\")\n",
    "except glue_client.exceptions.EntityNotFoundException:\n",
    "    print(\"Crawler does not exist or has already been deleted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wr.catalog.delete_database(name=DATABASE_NAME)\n",
    "    print(f\"Database {DATABASE_NAME} deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting database: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, verify that the resources have been deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.list_crawlers()\n",
    "print(\"Remaining crawlers:\", response['CrawlerNames'])\n",
    "\n",
    "databases = wr.catalog.databases()\n",
    "print(\"Remaining databases:\", databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well done!** In this lab, you implemented a simple data lake using Amazon S3, AWS Glue ETL, Glue Crawler, and Amazon Athena. You ingested raw data into an S3 bucket, which acted as the storage layer of the data lake. You then used AWS GLUE ETL to transform and prepare the data for analysis. After that, you used the Glue Crawler to catalog the data, and then used Athena to run SQL queries directly against the data lake without the need for complex data movement or transformation. This setup allows for easy and efficient querying of data stored in S3, enabling organizations to derive valuable insights from their data assets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
